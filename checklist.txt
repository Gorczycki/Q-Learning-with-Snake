//board is (0,16) in length, (0,14) in height
//meaning total board is 16*14

g++ -std=c++20 testing.cpp game.cpp computer2.cpp snake.cpp apple.cpp timer.cpp -o test.x

//having trouble defining which cell to fill when growing the snake, it is dependent on the direction
//fix by checking direction of the preceding two body cells, if(snake[2] == snake[1] minus an x value, we know we are in a 
certain direction)

//is the snake head the first index in body[] or the last? --> snake head is body[0], so everything is reversed

//note, it is not possible for the snake to grow diagonally, so we are safe with the grow
//function as it is

//std::cout<<test.back().first<<" "<<test.back().second; is the tail, so snake printed in reverse?
//std::cout<<test.front().first<<" "<<test.front().second; is the head.

//cout prints tail to head or head to tail?

//snake_grow looks good now

//shift_snake looks good now


5,8
5,7
4,7

becomes

6,8
5,8
5,7

another:

5,8
5,7
4,7
3,7

becomes

6,8
5,8
5,7
4,7

//snake.cpp will need a function to grow, shift, take in direction from computer,
//snake.cpp will not need a function for collision, or apple detection, or something for the other files

//should the master game.cpp handle all of the booleans? for example defining the radius threshold to spawn the apple.
//then game.cpp will have an if statement for apple eaten, thus prompting void place_apple() until radius works

//we now have snake.cpp, apple.cpp, and timer.cpp. now test apple.cpp
//with vector entry, it seems we have backward values for (x,y)

----------------------------------------------------------------------------------------------------------------------------------------------------------------
//Now for game.cpp:
//not sure how to bring in direction changes
//do we need a start_time() function?

//game.cpp is working fine now without computer

//now to learn about learning

----------------------------------------------------------------------------------------------------------------------------------------------------------------

will need a sleep_for on the direction changing from computer.

optimal path for the snake will always be towards the apple. However we need to account for avoiding a self-collision or wall collision.
We do not have an analytic solution for the snake game due to the apple's randomness in placement. Therefore body orientation is not a solved
case. 

A is a set of actions called the action space, S is a set of states called the state space. A policy function \pi is a probabilistic
mapping from S to A., i.e., a_t = \pi (s_t) where a \in A, s_t \in S. We want to find optimal policy.

Snake is a fully-observable game, so Bayesian RL is not a good fit. We will choose Q-Learning, using the Bellman equation, and also use epsilon-greedy 
strategy, meaning it randomly explores with probability \epsilon and exploits current knowledge otherwise. Given this randomness, we may compare
using std::mt19937 versus van der corput or halton or otherwise. Does Q-learning allow for full observability of the game?

WE COULD USE MONTE CARLO METHODS?

--> Monte carlo only learns from states that would appear in a prior game. It estimates state-action values 
Q(s,a) by averaging the rewards received for taking action a in state s. Will also need a wrong_move function for if we
are heading right but computer chooses left

moved char direction in snake from private to public, we'll see if working. I want game.cpp to be the medium between computer.cpp and snake.cpp

//now need to check for "bad" moves, which are not allowed in regular game. such as computer sending left when already travelling right

will have {state_x, move_dir, reward}

Linear function approximation:

Q(s,a) = w_1f_1 + w_2f_2 + ... w_nf_n

w_i = w_i + \alpha(G - Q(s,a))f_i

after playing an episode, update weights using w_i = w_i + \alpha(G - Q(s,a))f_i
where G is the sum of rewards from a state-action pair, \alpha is learning rate
G_t = R_{t+1} + R_{t+2} + ... + R_T

collect an episode into vectors, and then push them to csv's. then clear vectors. 
Weights correspond to their feature.

My reinforcement learning model consists of:
- policy
- weights
- rewards
- features
- learning rate
- epsilon greedy choice

the policy chooses an action,
the game gives a reward
features describe the state
The Q-Value is computed from the features and weights
At the end of an episode, the weights are updated using monte carlo returns.

Q_values of size 4 has elements of which the highest is chosen
Each element of Q_values is computed from w_i*f_i

at the end of each epsiode, calculate total return for each state-action pair

void Game::set_board()
{
    board_reset();
    for(int i = 0; i<snake.get_body().size(); i++)
    {
        board[snake.get_body()[i].second][snake.get_body()[i].first] = 1;
    }

    //board[snake.get_head().second][snake.get_head().first] = 3;
    //for(int i = 1; i<snake.get_body().size(); i++)
    //{
    //    board[snake.get_body()[i].second][snake.get_body()[i].first] = 1;
    //}
    board[apple.get_apple_loc().second][apple.get_apple_loc().first] = 2;
}

----------------------------------------------------------------------------------------------------------------------------------------------------------------

Attempt 2:

a state s may be distance from snake's head to apple, as well as distance from snake's head to tail. The optimal way to play snake is to leave enough distance such that
snake's head is not in danger of running into itself, or creating a small boundary with it's body.

With a certain probability, we want the snake to move in such a way that the distance between snake head and apple decreases
This should be a higher probability, we want this action to be performed 8/10's chance.

We cant just hard-code self-collision avoidance, this is where we need to bring in penalties!
To begin penalties for self collision, wall collision, we need to create a state table
A state will consist of distances from head to each body part. we will make it such that more emphasis
is numerically placed on body parts closest to head, emphasis decreasing slightly as we move further towards tail from head

self collision is working, so most of these games are ending due to self-collision

4/1/25:
Want to implement avoidance of self-collision. We are currently experiencing self-collision as
the current function only seeks to minimize the distance from the snake's head to the apple, dragging the body along with it.
Then the snake will seek to cross it's own body as it moves towards the apple. A solution will be to
hard-code avoidance of self-collision. The learning will come in as we seek to crowd the snake into the minimum
amount of cells, as in we define an imaginary rectangular area defined from width and height and then seek
to minimize the actual area compared to imaginary area.

update: actually, hard-coding self-avoidance is a bit of a cheat, as what if in a late stage game the snake traps itself?
then it will have no choice. In the current implementation we are hard-coding a move that minimizes radius. 
Is this fine for learning? can we build the learning around this fact? Maybe. if we somehow 
compare the hard-code move against a move that sacrifices current positioning for better future 
positioning. One would choose a sacrifice against current minimization in the case that a self-collision would occur,
or to better fold the snake into a confined area. To begin comparison, we could make a copy of the snake body
and then shift it under the hard-coded min move. We then check this shift for self-collision. 
The current hard-coded min move is a policy.

-remove epsilon in Computer::decision()


g++ -o your_program your_program.cpp -lsfml-graphics -lsfml-window -lsfml-system

g++ -std=c++20 testing.cpp game.cpp computer2.cpp snake.cpp apple.cpp timer.cpp -o test.x -lsfml-graphics -lsfml-window -lsfml-system

g++ -std=c++20 testing.cpp game.cpp computer2.cpp snake.cpp apple.cpp timer.cpp -o test.x -I/opt/homebrew/Cellar/sfml/3.0.0_1/include -L/opt/homebrew/Cellar/sfml/3.0.0_1/lib -lsfml-graphics -lsfml-window -lsfml-system

 python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install xyz

g++ -std=c++20 deleter.cpp -o jim.x

function to show best game over 100 games, count apples eaten in a single game. Implemented by making a 3-tensor of a games' frames. if previous tensor had smaller 
amount of elements, take current tensor, repeat. maybe not.
want to instead count number of frames in current game, if current frames at end of current game is greater than max frames, this currentg
game is the best game. Could make two txt files, one for current game and one for previous game, max.txt and pusher.txt. max.txt is initialized to zero,
and pusher.txt takes first game and pushes it to max.txt. clear pusher.txt and then send game 2 to pusher.txt, check line count between pusher(game 2) and max.txt, 
if pusher has more lines, clear max.txt and send pusher to max.txt, clear pusher for future game 3.

in void Game::update_board() having std::this_thread::sleep_for(std::chrono::milliseconds(200)); i believe the seg-fault in Game::set_board() comes from having the milliseconds to low



while (!game_over) {

    Computer::State state = getCurrentState(); // 1. record state

    char action = computer.chooseAction(state); // 2. choose action

    moveSnake(action); // 3. take action (you already have move functions)

    float reward = getReward(); // 4. observe reward (apple eaten, death, step)
    
    Computer::State next_state = getCurrentState(); // 4. observe next state

    computer.updateQ(state, action, reward, next_state); // 5. update Q
}


